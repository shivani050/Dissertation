{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e335749-a8a2-44df-bce4-76467caa563c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.38.2\n",
      "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "     ---------------------------------------- 0.0/130.7 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/130.7 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/130.7 kB ? eta -:--:--\n",
      "     -------- ---------------------------- 30.7/130.7 kB 262.6 kB/s eta 0:00:01\n",
      "     ----------- ------------------------- 41.0/130.7 kB 196.9 kB/s eta 0:00:01\n",
      "     ----------------- ------------------- 61.4/130.7 kB 252.2 kB/s eta 0:00:01\n",
      "     -------------------- ---------------- 71.7/130.7 kB 262.6 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 122.9/130.7 kB 379.3 kB/s eta 0:00:01\n",
      "     ------------------------------------ 130.7/130.7 kB 386.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from transformers==4.38.2) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.38.2)\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from transformers==4.38.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from transformers==4.38.2) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from transformers==4.38.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from transformers==4.38.2) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from transformers==4.38.2) (2.32.2)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.2)\n",
      "  Downloading tokenizers-0.15.2-cp312-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.38.2)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from transformers==4.38.2) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.38.2) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from requests->transformers==4.38.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from requests->transformers==4.38.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from requests->transformers==4.38.2) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from requests->transformers==4.38.2) (2025.4.26)\n",
      "Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "   ---------------------------------------- 0.0/8.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/8.5 MB 1.6 MB/s eta 0:00:06\n",
      "    --------------------------------------- 0.1/8.5 MB 2.1 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.2/8.5 MB 2.0 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.3/8.5 MB 2.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.4/8.5 MB 2.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.6/8.5 MB 2.2 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.7/8.5 MB 2.2 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.8/8.5 MB 2.3 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.9/8.5 MB 2.3 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.0/8.5 MB 2.4 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.2/8.5 MB 2.4 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.3/8.5 MB 2.5 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.5/8.5 MB 2.6 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.6/8.5 MB 2.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.8/8.5 MB 2.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.0/8.5 MB 2.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.1/8.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.3/8.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.4/8.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.5/8.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.6/8.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.7/8.5 MB 2.7 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.9/8.5 MB 2.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.0/8.5 MB 2.8 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.2/8.5 MB 2.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.4/8.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.6/8.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.8/8.5 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.0/8.5 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.2/8.5 MB 3.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.5/8.5 MB 3.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.7/8.5 MB 3.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.9/8.5 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.1/8.5 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.4/8.5 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.6/8.5 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.9/8.5 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.2/8.5 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.5/8.5 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.7/8.5 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.9/8.5 MB 3.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.2/8.5 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.4/8.5 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.7/8.5 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.0/8.5 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.3/8.5 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.5/8.5 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.5/8.5 MB 4.0 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "   ---------------------------------------- 0.0/514.8 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 256.0/514.8 kB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 514.8/514.8 kB 8.1 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "   ---------------------------------------- 0.0/308.9 kB ? eta -:--:--\n",
      "   ---------------------------------------  307.2/308.9 kB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 308.9/308.9 kB 6.5 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.2-cp312-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.4/2.2 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.7/2.2 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.1/2.2 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.4/2.2 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.7/2.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.1/2.2 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 6.9 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.33.0 safetensors-0.5.3 tokenizers-0.15.2 transformers-4.38.2\n",
      "Collecting torch\n",
      "  Using cached torch-2.7.1-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Using cached torch-2.7.1-cp312-cp312-win_amd64.whl (216.1 MB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.3 MB 325.1 kB/s eta 0:00:20\n",
      "   ---------------------------------------- 0.1/6.3 MB 363.1 kB/s eta 0:00:18\n",
      "    --------------------------------------- 0.1/6.3 MB 435.7 kB/s eta 0:00:15\n",
      "    --------------------------------------- 0.2/6.3 MB 654.6 kB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.2/6.3 MB 798.9 kB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.3/6.3 MB 933.7 kB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.4/6.3 MB 1.1 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.5/6.3 MB 1.2 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 0.6/6.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 0.7/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 0.8/6.3 MB 1.5 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.0/6.3 MB 1.6 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.1/6.3 MB 1.7 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.3/6.3 MB 1.8 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.4/6.3 MB 1.9 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 1.5/6.3 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 1.7/6.3 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.9/6.3 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.0/6.3 MB 2.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.2/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.4/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 2.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 2.7/6.3 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 2.9/6.3 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.1/6.3 MB 2.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.3/6.3 MB 2.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.6/6.3 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.8/6.3 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.0/6.3 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.2/6.3 MB 2.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.4/6.3 MB 3.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.6/6.3 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.8/6.3 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.1/6.3 MB 3.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.3/6.3 MB 3.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.6/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.8/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.1/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 3.4 MB/s eta 0:00:00\n",
      "Installing collected packages: sympy, torch\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "Successfully installed sympy-1.14.0 torch-2.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.38.2\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54e35f5d-989b-40a7-a84b-f9fcddf383e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.38.2\n",
      "Torch version: 2.7.1+cpu\n",
      "CUDA Available: False\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a70244d-6cf5-4d27-aec7-755ca3b594ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combined samples: 218534\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load tokenized data\n",
    "devign_df = pd.read_csv(\"devign_tokens.csv\")\n",
    "bigvul_df = pd.read_csv(\"bigvul_tokens.csv\")\n",
    "nvd_df = pd.read_csv(\"nvd_tokens.csv\")\n",
    "\n",
    "# Merge all labeled datasets\n",
    "combined_df = pd.concat([devign_df, bigvul_df, nvd_df], ignore_index=True)\n",
    "combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Total combined samples:\", len(combined_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a543190d-84da-4f89-b3a8-a12e36aef288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 157344 | Val: 17483 | Test: 43707\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(combined_df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fc19391-21f7-4889-b6ea-bad46f1f49d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class VulnDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = df[\"input_ids\"].apply(eval).tolist()\n",
    "        self.attn_mask = df[\"attention_mask\"].apply(eval).tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.input_ids[idx]),\n",
    "            \"attention_mask\": torch.tensor(self.attn_mask[idx]),\n",
    "            \"labels\": torch.tensor(self.labels[idx])\n",
    "        }\n",
    "\n",
    "train_dataset = VulnDataset(train_df)\n",
    "val_dataset = VulnDataset(val_df)\n",
    "test_dataset = VulnDataset(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3bc05e51-742a-49c9-81fd-852706a76b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\shivani\\anaconda3\\lib\\site-packages (24.0)\n",
      "Collecting pip\n",
      "  Using cached pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Using cached pip-25.1.1-py3-none-any.whl (1.8 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\Shivani\\anaconda3\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.52.4\n",
      "Uninstalling transformers-4.52.4:\n",
      "  Successfully uninstalled transformers-4.52.4\n",
      "Found existing installation: accelerate 1.7.0\n",
      "Uninstalling accelerate-1.7.0:\n",
      "  Successfully uninstalled accelerate-1.7.0\n",
      "Found existing installation: torch 2.7.1\n",
      "Uninstalling torch-2.7.1:\n",
      "  Successfully uninstalled torch-2.7.1\n",
      "Collecting transformers[torch]\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from transformers[torch]) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from transformers[torch]) (4.66.4)\n",
      "Collecting torch<2.7,>=2.1 (from transformers[torch])\n",
      "  Using cached torch-2.6.0-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.7.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from torch<2.7,>=2.1->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from torch<2.7,>=2.1->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from torch<2.7,>=2.1->transformers[torch]) (69.5.1)\n",
      "Collecting sympy==1.13.1 (from torch<2.7,>=2.1->transformers[torch])\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch<2.7,>=2.1->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from jinja2->torch<2.7,>=2.1->transformers[torch]) (2.1.3)\n",
      "Using cached torch-2.6.0-cp312-cp312-win_amd64.whl (204.1 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "Installing collected packages: sympy, torch, transformers\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.14.0\n",
      "    Uninstalling sympy-1.14.0:\n",
      "      Successfully uninstalled sympy-1.14.0\n",
      "Successfully installed sympy-1.13.1 torch-2.6.0 transformers-4.52.4\n",
      "Requirement already satisfied: torch in c:\\users\\shivani\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Collecting torch\n",
      "  Using cached torch-2.7.1-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shivani\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Using cached torch-2.7.1-cp312-cp312-win_amd64.whl (216.1 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Installing collected packages: sympy, torch\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0\n",
      "    Uninstalling torch-2.6.0:\n",
      "      Successfully uninstalled torch-2.6.0\n",
      "Successfully installed sympy-1.14.0 torch-2.7.1\n",
      "Name: accelerate\n",
      "Version: 1.7.0\n",
      "Summary: Accelerate\n",
      "Home-page: https://github.com/huggingface/accelerate\n",
      "Author: The HuggingFace team\n",
      "Author-email: zach.mueller@huggingface.co\n",
      "License: Apache\n",
      "Location: C:\\Users\\Shivani\\anaconda3\\Lib\\site-packages\n",
      "Requires: huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
      "Required-by: \n",
      "Name: transformers\n",
      "Version: 4.52.4\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: C:\\Users\\Shivani\\anaconda3\\Lib\\site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: \n",
      "Name: torch\n",
      "Version: 2.7.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3-Clause\n",
      "Location: C:\\Users\\Shivani\\anaconda3\\Lib\\site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, setuptools, sympy, typing-extensions\n",
      "Required-by: accelerate\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip uninstall -y transformers accelerate torch\n",
    "!pip install accelerate>=0.21.0 --no-deps\n",
    "!pip install transformers[torch] --upgrade\n",
    "!pip install torch --upgrade\n",
    "\n",
    "# Verify installations\n",
    "!pip show accelerate\n",
    "!pip show transformers\n",
    "!pip show torch\n",
    "\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51053bb5-0f9f-4d1c-9d88-979342235584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\shivani\\anaconda3\\lib\\site-packages (24.0)\n",
      "Collecting pip\n",
      "  Using cached pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Using cached pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.0\n",
      "    Uninstalling pip-24.0:\n",
      "      Successfully uninstalled pip-24.0\n",
      "Successfully installed pip-25.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fc714a4-cca7-4e29-a9fb-4dec4f6b73fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers>=4.0.0 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ec6f27f-b944-4345-884c-43da86f5963a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current transformers version: 4.52.4\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(f\"Current transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7456179-8a0c-4932-95db-887ea1e4369b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.52.4\n",
      "Available parameters for TrainingArguments:\n",
      "- output_dir\n",
      "- overwrite_output_dir\n",
      "- do_train\n",
      "- do_eval\n",
      "- do_predict\n",
      "- eval_strategy\n",
      "- prediction_loss_only\n",
      "- per_device_train_batch_size\n",
      "- per_device_eval_batch_size\n",
      "- per_gpu_train_batch_size\n",
      "- per_gpu_eval_batch_size\n",
      "- gradient_accumulation_steps\n",
      "- eval_accumulation_steps\n",
      "- eval_delay\n",
      "- torch_empty_cache_steps\n",
      "- learning_rate\n",
      "- weight_decay\n",
      "- adam_beta1\n",
      "- adam_beta2\n",
      "- adam_epsilon\n",
      "- max_grad_norm\n",
      "- num_train_epochs\n",
      "- max_steps\n",
      "- lr_scheduler_type\n",
      "- lr_scheduler_kwargs\n",
      "- warmup_ratio\n",
      "- warmup_steps\n",
      "- log_level\n",
      "- log_level_replica\n",
      "- log_on_each_node\n",
      "- logging_dir\n",
      "- logging_strategy\n",
      "- logging_first_step\n",
      "- logging_steps\n",
      "- logging_nan_inf_filter\n",
      "- save_strategy\n",
      "- save_steps\n",
      "- save_total_limit\n",
      "- save_safetensors\n",
      "- save_on_each_node\n",
      "- save_only_model\n",
      "- restore_callback_states_from_checkpoint\n",
      "- no_cuda\n",
      "- use_cpu\n",
      "- use_mps_device\n",
      "- seed\n",
      "- data_seed\n",
      "- jit_mode_eval\n",
      "- use_ipex\n",
      "- bf16\n",
      "- fp16\n",
      "- fp16_opt_level\n",
      "- half_precision_backend\n",
      "- bf16_full_eval\n",
      "- fp16_full_eval\n",
      "- tf32\n",
      "- local_rank\n",
      "- ddp_backend\n",
      "- tpu_num_cores\n",
      "- tpu_metrics_debug\n",
      "- debug\n",
      "- dataloader_drop_last\n",
      "- eval_steps\n",
      "- dataloader_num_workers\n",
      "- dataloader_prefetch_factor\n",
      "- past_index\n",
      "- run_name\n",
      "- disable_tqdm\n",
      "- remove_unused_columns\n",
      "- label_names\n",
      "- load_best_model_at_end\n",
      "- metric_for_best_model\n",
      "- greater_is_better\n",
      "- ignore_data_skip\n",
      "- fsdp\n",
      "- fsdp_min_num_params\n",
      "- fsdp_config\n",
      "- fsdp_transformer_layer_cls_to_wrap\n",
      "- accelerator_config\n",
      "- deepspeed\n",
      "- label_smoothing_factor\n",
      "- optim\n",
      "- optim_args\n",
      "- adafactor\n",
      "- group_by_length\n",
      "- length_column_name\n",
      "- report_to\n",
      "- ddp_find_unused_parameters\n",
      "- ddp_bucket_cap_mb\n",
      "- ddp_broadcast_buffers\n",
      "- dataloader_pin_memory\n",
      "- dataloader_persistent_workers\n",
      "- skip_memory_metrics\n",
      "- use_legacy_prediction_loop\n",
      "- push_to_hub\n",
      "- resume_from_checkpoint\n",
      "- hub_model_id\n",
      "- hub_strategy\n",
      "- hub_token\n",
      "- hub_private_repo\n",
      "- hub_always_push\n",
      "- gradient_checkpointing\n",
      "- gradient_checkpointing_kwargs\n",
      "- include_inputs_for_metrics\n",
      "- include_for_metrics\n",
      "- eval_do_concat_batches\n",
      "- fp16_backend\n",
      "- push_to_hub_model_id\n",
      "- push_to_hub_organization\n",
      "- push_to_hub_token\n",
      "- mp_parameters\n",
      "- auto_find_batch_size\n",
      "- full_determinism\n",
      "- torchdynamo\n",
      "- ray_scope\n",
      "- ddp_timeout\n",
      "- torch_compile\n",
      "- torch_compile_backend\n",
      "- torch_compile_mode\n",
      "- include_tokens_per_second\n",
      "- include_num_input_tokens_seen\n",
      "- neftune_noise_alpha\n",
      "- optim_target_modules\n",
      "- batch_eval_metrics\n",
      "- eval_on_start\n",
      "- use_liger_kernel\n",
      "- eval_use_gather_object\n",
      "- average_tokens_across_devices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying to create TrainingArguments with minimal parameters...\n",
      "Success!\n",
      "\n",
      "Trying with full parameters...\n",
      "Full parameters worked!\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect the actual TrainingArguments class to see what parameters it accepts\n",
    "import transformers\n",
    "from transformers import TrainingArguments\n",
    "import inspect\n",
    "\n",
    "# Print the version again to confirm\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "\n",
    "# Get the signature of the TrainingArguments class\n",
    "sig = inspect.signature(TrainingArguments.__init__)\n",
    "print(\"Available parameters for TrainingArguments:\")\n",
    "for param_name, param in sig.parameters.items():\n",
    "    if param_name != 'self' and param_name != 'kwargs':\n",
    "        print(f\"- {param_name}\")\n",
    "\n",
    "# Now let's create the training arguments based on what's actually available\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "# Create the model\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"microsoft/codebert-base\", num_labels=2)\n",
    "\n",
    "# Create a minimal set of arguments first to test\n",
    "print(\"\\nTrying to create TrainingArguments with minimal parameters...\")\n",
    "try:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./vuln_model\",\n",
    "    )\n",
    "    print(\"Success!\")\n",
    "    \n",
    "    # Now try with the full set of parameters we want\n",
    "    print(\"\\nTrying with full parameters...\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./vuln_model\",\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        # Try each parameter one by one\n",
    "        # Uncomment these as needed\n",
    "        # evaluation_strategy=\"epoch\",\n",
    "        # save_strategy=\"epoch\",\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=100,\n",
    "        # load_best_model_at_end=True,\n",
    "        # metric_for_best_model=\"eval_loss\",\n",
    "    )\n",
    "    print(\"Full parameters worked!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please check the available parameters listed above and adjust accordingly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cf006d2-99d0-4282-957f-73cdb7229794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments created successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Create the model\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"microsoft/codebert-base\", num_labels=2)\n",
    "\n",
    "# Create training arguments with the correct parameter names\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vuln_model\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"epoch\",  # Changed from evaluation_strategy to eval_strategy\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "print(\"Training arguments created successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb8e93-101d-475e-afbf-6065890c6ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shivani\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='567' max='59004' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  567/59004 1:07:59 < 117:12:34, 0.14 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
